
@Article{ElemNet,
author={Jha, Dipendra
and Ward, Logan
and Paul, Arindam
and Liao, Wei-keng
and Choudhary, Alok
and Wolverton, Chris
and Agrawal, Ankit},
title={ElemNet: Deep Learning the Chemistry of Materials From Only Elemental Composition},
journal={Scientific Reports},
year={2018},
volume={8},
number={1},
pages={17593},
abstract={Conventional machine learning approaches for predicting material properties from elemental compositions have emphasized the importance of leveraging domain knowledge when designing model inputs. Here, we demonstrate that by using a deep learning approach, we can bypass such manual feature engineering requiring domain knowledge and achieve much better results, even with only a few thousand training samples. We present the design and implementation of a deep neural network model referred to as ElemNet; it automatically captures the physical and chemical interactions and similarities between different elements using artificial intelligence which allows it to predict the materials properties with better accuracy and speed. The speed and best-in-class accuracy of ElemNet enable us to perform a fast and robust screening for new material candidates in a huge combinatorial space; where we predict hundreds of thousands of chemical systems that could contain yet-undiscovered compounds.},
issn={2045-2322},
doi={10.1038/s41598-018-35934-y},
url={https://doi.org/10.1038/s41598-018-35934-y}
}
@Article{Schmidt2017,
author={Schmidt, Jonathan
and Shi, Jingming
and Borlido, Pedro
and Chen, Liming
and Botti, Silvana
and Marques, Miguel A. L.},
title={Predicting the Thermodynamic Stability of Solids Combining Density Functional Theory and Machine Learning},
journal={Chemistry of Materials},
year={2017},
month={Jun},
day={27},
publisher={American Chemical Society},
volume={29},
number={12},
pages={5090-5103},
issn={0897-4756},
doi={10.1021/acs.chemmater.7b00156},
url={https://doi.org/10.1021/acs.chemmater.7b00156}
}

@inproceedings{Finocchi2011DensityFT,
  title={Density Functional Theory for Beginners Basic Principles and Practical Approaches},
  author={Fabio Finocchi},
  year={2011}
}
@article{PhysRev.140.A1133,
  title = {Self-Consistent Equations Including Exchange and Correlation Effects},
  author = {Kohn, W. and Sham, L. J.},
  journal = {Phys. Rev.},
  volume = {140},
  issue = {4A},
  pages = {A1133--A1138},
  numpages = {0},
  year = {1965},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.140.A1133},
  url = {https://link.aps.org/doi/10.1103/PhysRev.140.A1133}
}
@article{PhysRev.136.B864,
  title = {Inhomogeneous Electron Gas},
  author = {Hohenberg, P. and Kohn, W.},
  journal = {Phys. Rev.},
  volume = {136},
  issue = {3B},
  pages = {B864--B871},
  numpages = {0},
  year = {1964},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.136.B864},
  url = {https://link.aps.org/doi/10.1103/PhysRev.136.B864}
}
@inproceedings{Pytorch,
  title={Automatic differentiation in PyTorch},
  author={Adam Paszke and Sam Gross and Soumith Chintala and Gregory Chanan and Edward Yang and Zachary Devito and Zeming Lin and Alban Desmaison and Luca Antiga and Adam Lerer},
  year={2017}
}
@InProceedings{deng2013recent,
author = {Deng, Li and Li, Jinyu and Huang, Jui-Ting and Yao, Kaisheng and Yu, Dong and Seide, Frank and Seltzer, Mike and Zweig, Geoff and He, Xiaodong and Williams, Jason and Gong, Yifan and Acero, Alex},
title = {Recent Advances in Deep Learning for Speech Research at Microsoft},
year = {2013},
month = {May},
abstract = {Deep learning is becoming a mainstream technology for speech recognition at industrial scale. In this paper, we provide an overview of the work by Microsoft speech researchers since 2009 in this area, focusing on more recent advances which shed light to the basic capabilities and limitations of the current deep learning technology. We organize this overview along the feature-domain and model-domain dimensions according to the conventional approach to analyzing speech systems. Selected experimental results, including speech recognition and related applications such as spoken dialogue and language modeling, are presented to demonstrate and analyze the strengths and weaknesses of the techniques described in the paper. Potential improvement of these techniques and future research directions are discussed.},
publisher = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
url = {https://www.microsoft.com/en-us/research/publication/recent-advances-in-deep-learning-for-speech-research-at-microsoft/},
}
@article{DBLP:journals/corr/SzegedyIV16,
  author    = {Christian Szegedy and
               Sergey Ioffe and
               Vincent Vanhoucke},
  title     = {Inception-v4, Inception-ResNet and the Impact of Residual Connections
               on Learning},
  journal   = {CoRR},
  volume    = {abs/1602.07261},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.07261},
  archivePrefix = {arXiv},
  eprint    = {1602.07261},
  timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyIV16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Biophysics,
author = {Buccino, Alessio P. and Kordovan, Michael and Ness, Torbjørn V. and Merkt, Benjamin and Häfliger, Philipp D. and Fyhn, Marianne and Cauwenberghs, Gert and Rotter, Stefan and Einevoll, Gaute T.},
title = {Combining biophysical modeling and deep learning for multielectrode array neuron localization and classification},
journal = {Journal of Neurophysiology},
volume = {120},
number = {3},
pages = {1212-1232},
year = {2018},
doi = {10.1152/jn.00210.2018},
    note ={PMID: 29847231},

URL = { 
        https://doi.org/10.1152/jn.00210.2018
    
},
eprint = { 
        https://doi.org/10.1152/jn.00210.2018
    
}
,
    abstract = { Neural circuits typically consist of many different types of neurons, and one faces a challenge in disentangling their individual contributions in measured neural activity. Classification of cells into inhibitory and excitatory neurons and localization of neurons on the basis of extracellular recordings are frequently employed procedures. Current approaches, however, need a lot of human intervention, which makes them slow, biased, and unreliable. In light of recent advances in deep learning techniques and exploiting the availability of neuron models with quasi-realistic three-dimensional morphology and physiological properties, we present a framework for automatized and objective classification and localization of cells based on the spatiotemporal profiles of the extracellular action potentials recorded by multielectrode arrays. We train convolutional neural networks on simulated signals from a large set of cell models and show that our framework can predict the position of neurons with high accuracy, more precisely than current state-of-the-art methods. Our method is also able to classify whether a neuron is excitatory or inhibitory with very high accuracy, substantially improving on commonly used clustering techniques. Furthermore, our new method seems to have the potential to separate certain subtypes of excitatory and inhibitory neurons. The possibility of automatically localizing and classifying all neurons recorded with large high-density extracellular electrodes contributes to a more accurate and more reliable mapping of neural circuits.NEW \& NOTEWORTHY We propose a novel approach to localize and classify neurons from their extracellularly recorded action potentials with a combination of biophysically detailed neuron models and deep learning techniques. Applied to simulated data, this new combination of forward modeling and machine learning yields higher performance compared with state-of-the-art localization and classification methods. }
}
@misc{Particlephysics,
    title={Machine Learning in High Energy Physics Community White Paper},
    author={Kim Albertsson and Piero Altoe and Dustin Anderson and John Anderson and Michael Andrews and Juan Pedro Araque Espinosa and Adam Aurisano and Laurent Basara and Adrian Bevan and Wahid Bhimji and Daniele Bonacorsi and Bjorn Burkle and Paolo Calafiura and Mario Campanelli and Louis Capps and Federico Carminati and Stefano Carrazza and Yi-fan Chen and Taylor Childers and Yann Coadou and Elias Coniavitis and Kyle Cranmer and Claire David and Douglas Davis and Andrea De Simone and Javier Duarte and Martin Erdmann and Jonas Eschle and Amir Farbin and Matthew Feickert and Nuno Filipe Castro and Conor Fitzpatrick and Michele Floris and Alessandra Forti and Jordi Garra-Tico and Jochen Gemmler and Maria Girone and Paul Glaysher and Sergei Gleyzer and Vladimir Gligorov and Tobias Golling and Jonas Graw and Lindsey Gray and Dick Greenwood and Thomas Hacker and John Harvey and Benedikt Hegner and Lukas Heinrich and Ulrich Heintz and Ben Hooberman and Johannes Junggeburth and Michael Kagan and Meghan Kane and Konstantin Kanishchev and Przemysław Karpiński and Zahari Kassabov and Gaurav Kaul and Dorian Kcira and Thomas Keck and Alexei Klimentov and Jim Kowalkowski and Luke Kreczko and Alexander Kurepin and Rob Kutschke and Valentin Kuznetsov and Nicolas Köhler and Igor Lakomov and Kevin Lannon and Mario Lassnig and Antonio Limosani and Gilles Louppe and Aashrita Mangu and Pere Mato and Narain Meenakshi and Helge Meinhard and Dario Menasce and Lorenzo Moneta and Seth Moortgat and Mark Neubauer and Harvey Newman and Sydney Otten and Hans Pabst and Michela Paganini and Manfred Paulini and Gabriel Perdue and Uzziel Perez and Attilio Picazio and Jim Pivarski and Harrison Prosper and Fernanda Psihas and Alexander Radovic and Ryan Reece and Aurelius Rinkevicius and Eduardo Rodrigues and Jamal Rorie and David Rousseau and Aaron Sauers and Steven Schramm and Ariel Schwartzman and Horst Severini and Paul Seyfert and Filip Siroky and Konstantin Skazytkin and Mike Sokoloff and Graeme Stewart and Bob Stienen and Ian Stockdale and Giles Strong and Wei Sun and Savannah Thais and Karen Tomko and Eli Upfal and Emanuele Usai and Andrey Ustyuzhanin and Martin Vala and Justin Vasel and Sofia Vallecorsa and Mauro Verzetti and Xavier Vilasís-Cardona and Jean-Roch Vlimant and Ilija Vukotic and Sean-Jiun Wang and Gordon Watts and Michael Williams and Wenjing Wu and Stefan Wunsch and Kun Yang and Omar Zapata},
    year={2018},
    eprint={1807.02876},
    archivePrefix={arXiv},
    primaryClass={physics.comp-ph}
}
@article{OQMD,
title = "Materials design and discovery with high-throughput density functional theory: The open quantum materials database (OQMD)",
abstract = "High-throughput density functional theory (HT DFT) is fast becoming a powerful tool for accelerating materials design and discovery by the amassing tens and even hundreds of thousands of DFT calculations in large databases. Complex materials problems can be approached much more efficiently and broadly through the sheer quantity of structures and chemistries available in such databases. Our HT DFT database, the Open Quantum Materials Database (OQMD), contains over 200,000 DFT calculated crystal structures and will be freely available for public use at http://oqmd.org. In this review, we describe the OQMD and its use in five materials problems, spanning a wide range of applications and materials types: (I) Li-air battery combination catalyst/electrodes, (II) Li-ion battery anodes, (III) Li-ion battery cathode coatings reactive with HF, (IV) Mg-alloy long-period stacking ordered (LPSO) strengthening precipitates, and (V) training a machine learning model to predict new stable ternary compounds.",
author = "Saal, {James E.} and Scott Kirklin and Muratahan Aykol and Bryce Meredig and Wolverton, {Christopher M}",
year = "2013",
month = "11",
day = "1",
doi = "10.1007/s11837-013-0755-4",
language = "English (US)",
volume = "65",
pages = "1501--1509",
journal = "JOM",
issn = "1047-4838",
publisher = "Minerals, Metals and Materials Society",
number = "11",
}
@BOOK{Simonhaykin,
  TITLE = {Neural Networks: A Comprehensive Foundation},
  SUBTITLE = {A Comprehensive Foundation},
  AUTHOR = {Haykin, Simon},
  YEAR = {1998}, % In lecture notes
  PUBLISHER = {Pearson},
}
@misc{Lecturenotes,
  author        = {Ohlsson, Mattias and Edén, Patrik},
  title         = {Lecture Notes on Introduction to Artificial Neural Netwroks and Deep Learning},
  year          = {2019},
  publisher={Department of Astronomy and Theoretical Physics Lund University}
}
@book{Deeplearningbook,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@Article{McCulloch1943,
author={McCulloch, Warren S.
and Pitts, Walter},
title={A logical calculus of the ideas immanent in nervous activity},
journal={The bulletin of mathematical biophysics},
year={1943},
volume={5},
number={4},
pages={115-133},
abstract={Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
issn={1522-9602},
doi={10.1007/BF02478259},
url={https://doi.org/10.1007/BF02478259}
}

@article{CNN3Dinput,
title = "Multi-channel convolutional neural networks for materials properties prediction",
journal = "Computational Materials Science",
volume = "173",
pages = "109436",
year = "2020",
issn = "0927-0256",
doi = "https://doi.org/10.1016/j.commatsci.2019.109436",
url = "http://www.sciencedirect.com/science/article/pii/S0927025619307359",
author = "Xiaolong Zheng and Peng Zheng and Liang Zheng and Yang Zhang and Rui-Zhi Zhang",
keywords = "Convolutional neural networks, Material informatics, Elpasolite, Deep learning",
abstract = "Deep convolution neural networks (ConvNets) have been recently used to predict the enthalpy of formation and the prediction errors are within DFT precision. Here we show that a multi-channel input for the ConvNets improves the prediction accuracy, and the accuracy can be further improved by decomposing the input signals into high/low frequencies. We trained ConvNets using the periodic table representation on a DFT formation enthalpy dataset of 10,590 elpasolite compounds. The mean absolute error (MAE) reaches 50 meV/atom, which is half value of the MAE of a ConvNet using single input channel. The dependence of MAE on each element was also analyzed. Our work demonstrates the importance of input data preprocessing for ConvNets prediction accuracy in material informatics tasks."
}
@Article{CNN2D,
author ="Zheng, Xiaolong and Zheng, Peng and Zhang, Rui-Zhi",
title  ="Machine learning material properties from the periodic table using convolutional neural networks",
journal  ="Chem. Sci.",
year  ="2018",
volume  ="9",
issue  ="44",
pages  ="8426-8432",
publisher  ="The Royal Society of Chemistry",
doi  ="10.1039/C8SC02648C",
url  ="http://dx.doi.org/10.1039/C8SC02648C",
abstract  ="In recent years{,} convolutional neural networks (CNNs) have achieved great success in image recognition and shown powerful feature extraction ability. Here we show that CNNs can learn the inner structure and chemical information in the periodic table. Using the periodic table as representation{,} and full-Heusler compounds in the Open Quantum Materials Database (OQMD) as training and test samples{,} a multi-task CNN was trained to output the lattice parameter and enthalpy of formation simultaneously. The mean prediction errors were within DFT precision{,} and the results were much better than those obtained using only Mendeleev numbers or a random-element-positioning table{,} indicating that the two-dimensional inner structure of the periodic table was learned by the CNN as useful chemical information. Transfer learning was then utilized by fine-tuning the weights previously initialized on the OQMD training set. Using compounds with formula X2YZ in the Inorganic Crystal Structure Database (ICSD) as a second training set{,} the stability of full-Heusler compounds was predicted by using the fine-tuned CNN{,} and tungsten containing compounds were identified as rarely reported but potentially stable compounds."}
